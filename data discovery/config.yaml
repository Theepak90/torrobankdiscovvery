discovery:
  # Cloud Providers
  aws:
    enabled: false
    access_key: ""
    secret_key: ""
    region: "us-east-1"
    services: ["s3", "rds", "dynamodb", "redshift", "athena", "glue"]
  
  azure:
    enabled: false
    subscription_id: ""
    tenant_id: ""
    client_id: ""
    client_secret: ""
    services: ["blob_storage", "sql_database", "cosmos_db", "synapse", "data_factory", "event_hub", "service_bus"]
  
  gcp:
    enabled: false
    project_id: ""
    service_account_json: ""
    # For production use, set service_account_json to your actual GCP service account JSON
    # Example: service_account_json: '{"type": "service_account", "project_id": "your-project-id", ...}'
    services: ["bigquery"]
  
  oracle_cloud:
    enabled: false
    tenancy_id: ""
    user_id: ""
    fingerprint: ""
    private_key_path: ""
    region: "us-ashburn-1"
    services: ["object_storage", "autonomous_database", "mysql"]
  
  ibm_cloud:
    api_key: ""
    region: "us-south"
    services: ["object_storage", "db2", "cloudant", "watson_discovery"]
  
  alibaba_cloud:
    access_key_id: ""
    access_key_secret: ""
    region: "us-east-1"
    services: ["oss", "rds", "polardb", "mongodb"]
  
  # Data Lakes
  data_lakes:
    data_lake_connections: null
    enabled: false
  
  # Data Warehouses
  data_warehouses:
    enabled: false
    warehouse_connections: null
  
  # Databases
  databases:
    connection_timeout: 30
    database_connections: null
    enabled: false
  
  # File System
  file_system:
    enabled: true
    scan_paths:
    - "/tmp"
    - "/var/log"
    exclude_patterns:
    - "*.log"
    - "*.tmp"
    - ".DS_Store"
    max_file_size_mb: 100
  
  # SaaS Platforms
  saas_platforms:
    enabled: false
    platform_connections: null
  
  # Streaming
  streaming:
    enabled: false
    streaming_connections: null

  # BigQuery Connector (dedicated)
  bigquery:
    enabled: false
    project_id: ""
    # service_account_json: '{"type": "service_account", "project_id": "..."}'
    # credentials_path: "service-account.json"
    # dataset_id: ""  # Optional: specify a specific dataset to scan

# Monitoring
monitoring:
  enabled: true
  metrics_retention_days: 30
  health_check_interval: 300

# Logging
logging:
  level: INFO
  file: "data_discovery.log"
  max_size_mb: 10
  backup_count: 5
